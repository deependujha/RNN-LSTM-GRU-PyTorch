{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## RNN model\n",
    "\n",
    "![rnn name classification model](../assets/LSTM.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    # implement RNN from scratch rather than using nn.RNN\n",
    "    def __init__(self, input_size, hidden_size, output_size, no_of_layers=10):\n",
    "        super(LSTM, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.no_of_layers = no_of_layers # max number of letters in a word (layers in RNN)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=no_of_layers, # max number of letters in a word (layers in RNN)\n",
    "        )\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "\n",
    "    def forward(self, input_tensor, hidden_tensor, current_tensor):\n",
    "        # combined = torch.cat((input_tensor, hidden_tensor), 1)\n",
    "\n",
    "        output, hidden = self.lstm(input_tensor, (hidden_tensor, current_tensor))\n",
    "        output=output[-1]\n",
    "        output = self.linear(output)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden_n_current(self):\n",
    "        lstm_zero_tensor = torch.zeros(self.no_of_layers, self.hidden_size)\n",
    "        return lstm_zero_tensor.to(device), lstm_zero_tensor.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import ALL_LETTERS, N_LETTERS\n",
    "from utils import (\n",
    "    load_data,\n",
    "    letter_to_tensor,\n",
    "    line_to_tensor,\n",
    "    random_training_example,\n",
    ")\n",
    "\n",
    "category_lines, all_categories = load_data()\n",
    "n_categories = len(all_categories)\n",
    "\n",
    "print(\"n_categories:\", n_categories)\n",
    "print(\"n_letters:\", N_LETTERS)\n",
    "print(\"all_categories:\", all_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden = 128\n",
    "lstm = LSTM(N_LETTERS, n_hidden, n_categories).to(device)\n",
    "lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn(15, 57).to(device)\n",
    "hidden, current = lstm.init_hidden_n_current()\n",
    "print(f\"{input.shape=}\")\n",
    "print(f\"{hidden.shape=}\")\n",
    "output, next_hidden = lstm(input, hidden, current)\n",
    "print(f\"{output.shape=}\")\n",
    "print(f\"{next_hidden[0].shape=}\")\n",
    "print(f\"{next_hidden[1].shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_tensor, current_tensor = lstm.init_hidden_n_current()\n",
    "print(f\"{hidden_tensor.size()=}\")\n",
    "print(f\"{current_tensor.size()=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = line_to_tensor(\"Albert\")\n",
    "print(f\"{input_tensor.size()=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will convert string of any length to fixed length of 20 by adding spaces at the start\n",
    "# the reason for padding at the start is that, for smaller letters, we want to keep the information at the end\n",
    "\n",
    "def pad_string(name, max_length=10):\n",
    "    name = name.lower()\n",
    "    name = name.strip()\n",
    "    if len(name) > max_length:\n",
    "        return name[:max_length]\n",
    "    return name\n",
    "\n",
    "print(pad_string(\"Albert\"))\n",
    "print(pad_string(\"Albert\").__len__())\n",
    "print(pad_string(\"Deependu\").__len__())\n",
    "print(pad_string(\"Deependu\"))\n",
    "print(pad_string(\"albertaiolsofshfishoifssjfisjiofjso\").__len__())\n",
    "print(pad_string(\"albertaiolsofshfishoifssjfisjiofjso\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert them to tensors\n",
    "new_name = pad_string(\"Albert\")\n",
    "tensor_new_name = line_to_tensor(new_name)\n",
    "tensor_new_name.squeeze_()\n",
    "tensor_new_name.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_with_zero_tensor(tensor, max_length=10):\n",
    "    if tensor.size(0) < max_length:\n",
    "        zero_tensor = torch.zeros(max_length - tensor.size(0), N_LETTERS)\n",
    "        tensor = torch.cat((zero_tensor,tensor))\n",
    "    return tensor\n",
    "\n",
    "# a function to convert any random to string to input required for our model\n",
    "def name_to_input(_str_name):\n",
    "    _name = pad_string(_str_name)\n",
    "    # print(f\"{_name=}\")\n",
    "    name_tensor = line_to_tensor(_name)\n",
    "    name_tensor.squeeze_()\n",
    "    name_tensor = fill_with_zero_tensor(name_tensor)\n",
    "    return name_tensor.to(device)\n",
    "\n",
    "\n",
    "def category_from_output(output):\n",
    "    category_idx = torch.argmax(output).item()\n",
    "    return all_categories[category_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_to_input(\"Deep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "learning_rate = 0.005\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_tensor, category_tensor):\n",
    "    hidden_tensor, current_tensor = lstm.init_hidden_n_current()\n",
    "\n",
    "    output, _ = lstm(input_tensor, hidden_tensor, current_tensor)\n",
    "\n",
    "    # one_hot_category = torch.zeros(n_categories, dtype=torch.long)\n",
    "    # one_hot_category[category_tensor[0]] = 1\n",
    "    # print(f\"{one_hot_category=}\")\n",
    "    loss = criterion(output, category_tensor[0].to(device))\n",
    "    # return -1,-1\n",
    "    optimizer.zero_grad() # zero the gradient\n",
    "    loss.backward() # backpropagation (and calculate the gradients)\n",
    "    optimizer.step() # update the weights (by the gradients calculated in the previous line)\n",
    "\n",
    "    return output, loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "current_loss = 0\n",
    "all_losses = []\n",
    "plot_steps, print_steps = 1000, 5000\n",
    "n_iters = 100000\n",
    "\n",
    "for i in tqdm(range(n_iters)):\n",
    "    category, line, category_tensor, line_tensor = random_training_example(\n",
    "        category_lines, all_categories\n",
    "    )\n",
    "    my_input_for_model = name_to_input(line)\n",
    "    my_output, loss = train(my_input_for_model, category_tensor)\n",
    "    current_loss += loss\n",
    "\n",
    "    guess = category_from_output(my_output)\n",
    "    # print(f\"{my_output=}\")\n",
    "    # print(f\"{guess=}\")\n",
    "    # print(f\"{torch.argmax(my_output).item()=}\")\n",
    "    \n",
    "    if i==20:\n",
    "        break\n",
    "\n",
    "    if (i + 1) % plot_steps == 0:\n",
    "        all_losses.append(current_loss / plot_steps)\n",
    "        current_loss = 0\n",
    "\n",
    "    if (i + 1) % print_steps == 0:\n",
    "        guess = category_from_output(my_output)\n",
    "        correct = \"CORRECT\" if guess == category else f\"WRONG ({category})\"\n",
    "        print(f\"{i+1} {(i+1)/n_iters*100} {loss:.4f} {line} / {guess} {correct}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(all_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(input_line):\n",
    "    print(f\"\\n> {input_line}\")\n",
    "    lstm.eval()\n",
    "    with torch.no_grad():\n",
    "        line_tensor = name_to_input(input_line)\n",
    "\n",
    "        hidden, current = lstm.init_hidden_n_current()\n",
    "\n",
    "        output, hidden = lstm(line_tensor, hidden, current)\n",
    "\n",
    "        guess = category_from_output(output)\n",
    "        print(guess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(\"Albert\")\n",
    "predict(\"Deependu\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
